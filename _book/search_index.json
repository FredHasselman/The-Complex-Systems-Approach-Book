[
["index.html", "The Complex Systems Approach to Behavioural Science A practical guide", " The Complex Systems Approach to Behavioural Science Fred Hasselman &amp; Maarten Wijnants 2017 - 2018 A practical guide Image from Grip on Complexity "],
["course-guide.html", "Course guide", " Course guide Complexity research transcends the boundaries between the classical scientific disciplines and is a hot topic in physics, mathematics, biology, economy as well as psychology and the life sciences and is collectively referred to as the Complexity Sciences. This course will discuss techniques that allow for the study of human behaviour from the perspective of the Complexity Sciences, specifically, the study of complex physical systems that are alive and display complex adaptive behaviour such as learning and development. Contrary to what the term “complex” might suggest, complexity research is often about finding simple models / explanations that are able to simulate a wide range of qualitatively different behavioural phenomena. “Complex” generally refers to the object of study: Complex systems are composed of many constituent parts that interact with one another across many different temporal and spatial scales to generate behaviour at the level of the system as a whole that can appear to be periodic, nonlinear, unstable or extremely persistent. The focus of many research designs and analyses is to quantify the degree of periodicity, nonlinearity, context sensitivity or resistance to perturbation by exploiting the fact that “everything is interacting” in complex systems. This requires a mathematical formalism and rules of scientific inference that are very different from the mathematics underlying traditional statistical analyses that assume “everything is NOT interacting” in order to be able to validly infer statistical regularities in a dataset and generalise them to a population. The complex systems approach to behavioural science often overlaps with the idiographical approach of the science of the individual, that is, the goal is not to generalise properties or regularities to universal or statistical laws that hold at the level of infinitely large populations, but to apply general principles and universal laws that govern the adaptive behaviour of all complex systems to study specific facts, about specific systems observed in specific contexts at a specific instant. The main focus of the course will be hands-on data-analysis and the main analytical tool we will use is R (if you are an expert: It is also possible to use Matlab for most of the assignments, let us know in advance). Practical sessions will follow after a lecture session in which a specific technique will be introduced. We will cover the following topics: Theoretical background of phase transitions (self-organised criticality) and synchronisation (coupling dynamics) in complex dynamical systems and networks. Simple models of linear and nonlinear dynamical behaviour (Linear &amp; logistic growth, Predator-Prey dynamics, Lorenz system, the chaos game); Analysis of long range dependence in time and trial series (Entropy, Relative roughness, Standardized Dispersion Analysis, Detrended Fluctuation Analysis). Quantification of temporal patterns in time and trial series including dyadic interactions (Phase Space Reconstruction, [Cross] Recurrence Quantification Analysis). Network analyses (Estimating symptom networks, calculating network based complexity measures) "],
["teaching-formats.html", "Teaching formats", " Teaching formats Each meeting starts with a lecture addressing the theoretical and methodological backgrounds of the practical applications that will be used in hands-on assignments during the practical sessions. Several meetings include a part where guest lecturers discuss the use of one or more techniques in their recent research. Preparation To prepare for each lecture students should read the assigned chapters in this book and contribute to the 7 discussion assignments. Participating in the discussion assignments is a required part of this course. At the end of the course we will check whether each student has posted at least one question or answer for each assignment. We will not judge the content of the posts, but in order to pass, we need at least 7 posts, one for each assignment. The answers will be discussed during the subsequent lecture. Using Cerego We have to introduce a lot of new terminology and to help students get acquainted with these terms, they can use Cerego. An invitation will be sent to all participants of the course. The list of term is also included as Appendix A. Test information Examination will be based on a final assignment and a check of participation in weekly discussions on blackboard (the content of contributions will not be evaluated). Specifically: To prepare for each lecture students read a contemporary research paper in which a complex systems approach is used to a phenomenon studied in behavioural science. Students are required to formulate questions about each paper, and to initiate a discussion with their fellow-students on Blackboard. Each week at least one post by each student is expected in the discussion forum. A final take-home assignment will be provided at the end of the course. Details will be discussed during the course. In general, the assignment will take about 2 days to complete, the time available to complete the assignment will be 1-2 weeks depending on the schedule. This course is for students of the Research Master Behavioural Science. Other Research Master students and PhD students interested in following the course ask for permission by emailing to rm@bsi.ru.nl until 3 weeks before the start of the course. If permission is granted, this will be emailed 2 weeks before the start of the course. Please confirm this mail! PhD students (RU and external) have to subscribe through http://www.ru.nl/socialewetenschappen/onderwijs/overig/aanschuifonderwijs This course is not available for Bachelor and Master students. "],
["learning-objectives.html", "Learning objectives", " Learning objectives Specific Students who followed this course will be able to: Critically evaluate whether their scientific inquiries can benefit from adopting theories, models, methods and analyses that were developed in the Complexity Sciences to study the structure and behaviour of complex adaptive systems. Understand the differences between using an independent-statistical-component-dominant causal ontology, versus an interdependent-dynamical-interaction-dominant approach to the scientific study of human behaviour. Understand and apply important terms to describe behavioural change and adaptation: Nonlinear dynamics (e.g. hysteresis), attractor state, order parameter, control parameter, state-space, phase-space, phase transition, self-organisation, emergence, synergies as coordinative structures. Simulate linear, nonlinear and coupled growth using simple mathematical models in Excel and R (or Matlab). Fit parameters of simple models of linear and nonlinear growth to real data in SPSS or R (or Matlab). Perform analyses on time and trial series of human performance and physiology that quantify the presence and nature of scaling relations (fractal geometry) in continuous or categorical data in R (or Matlab). Perform analyses on time and trial series of human performance and physiology that quantify the presence and nature of temporal patterns (recurrent trajectories in phase space) in continuous or categorical data data in R (or Matlab). Perform network analyses on datasets that may be considered static or dynamical representations of social networks, or symptom networks (psychopathology) in R. Understand the results from analyses in terms of early warning signals indicating a phase transition might be imminent. Understand the results from analyses in terms of synchronisation and coupling phenomena, e.g. “complexity matching” and “leading/following” behaviour. General At the end of this course, students have reached a level of understanding that will allow them to: Study relevant scientific literature using a complex systems approach to behavioural science. Getting help with using a complex systems approach in their own scientific inquiries, e.g. by being able to ask relevant questions to experts on a specific topic discussed during the course. Work through tutorials on more advanced topics that were not discussed during the course. Keep up with the continuous influx of new theoretical, methodological and empirical studies on applying the complex systems approach in the behavioural-, cognitive- and neurosciences. "],
["literature.html", "Literature", " Literature Main literature: Hasselman, F., &amp; Wijnants, M. (2018). A Complex Systems Approach to the Behavioural Sciences. A practical guide to basic theory, models, methods and analyses [this book] Rose, T. (2016). The end of average: How we succeed in a world that values sameness. Penguin UK. [also available in Dutch and many other languages] Selected chapters from these books will be made available to make a personal copy Friedenberg, J. (2009). Dynamical psychology: Complexity, self-organization and mind. ISCE Publishing. Kaplan, D., &amp; Glass, L. (2012). Understanding nonlinear dynamics. Springer Science &amp; Business Media. We als provide links to online materials on specific topics (Study Materials) that may provide additional explanation and information about key concepts. These materials are not obligatory, but highly recommended to study at least once. Notes about the online book and the assignments The texts in the chapters of this book are intended as a rough introductory guide to accompany the lectures, thet are still somewhat of a work in progress. We made everything as coherent as possible, sometimes though, you will notice a paragraph or chapter rather resembles a set of lecture notes instead of a self-contained text. Do not hesitate to let us know if you think anything is unclear or too far out of context for you to understand. An essential part of the course are the assignments that are available online The text inside these blocks provides important information about the course, the assignments, or the exam. The text inside these blocks provides examples, or, information about a topic you should pay close attentiont to and try to understand. The text inside these blocks provides a note, a comment, or observation. The content in these blocks are often questions about a topic, or, suggestions about connections between different topics discussed in the book and the assignments. You should decide for yourself if you need to dig deeper to answer the questions or if you want to discuss the content. One way to find an answer or start a discussion is to open a thread in the discussion forum on Blackboard labelled ThinkBox. The content in these blocks is provided as entertainment :) "],
["schedule.html", "Schedule", " Schedule The dates and locations can be found below. All lectures are on Thursday from 10.45 to 12.30. The practical sessions take place on Thursday from 13.45 to 15.30. "],
["we-use-r.html", "We use R!", " We use R! This text was transformed to HTML, PDF en ePUB using bookdown(Xie, 2016a) in RStudio, the graphical user interface of the statistical language R (R Core Team, 2016). bookdown makes use of the R version of markdown called Rmarkdown (Allaire et al., 2016), together with knitr (Xie, 2016c) and pandoc. We’ll use some web applications made in Shiny (Chang, Cheng, Allaire, Xie, &amp; McPherson, 2016) Other R packages used are: DT (Xie, 2016b), htmlTable (Gordon, 2016), plyr (Wickham, 2016a), dplyr (Wickham &amp; Francois, 2016),tidyr (Wickham, 2016b), png (Urbanek, 2013), rio (Chan &amp; Leeper, 2016). "],
["a-quick-guide-to-scientific-rigour.html", "Chapter 1 A Quick Guide to Scientific Rigour", " Chapter 1 A Quick Guide to Scientific Rigour “Meanwhile our eager-beaver researcher, undismayed by logic-of-science considerations and relying blissfully on the “exactitude” of modern statistical hypothesis-testing, has produced a long publication list and been promoted to a full professorship. In terms of his contribution to the enduring body of psychological knowledge, he has done hardly anything. His true position is that of a potent-but-sterile intellectual rake, who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” —Paul Meehl (1967, p. 114) Before we can begin our introduction to the wonderful world of Complex Adaptive Systems and Complex Networks, we briefly discuss the philosophy of science and perspective on the goal of scientific inquiry that is used throughout this book. This will allow us to highlight some differences between the Complex Systems Approach (CSA) we propose for the scientific study of human nature and the classical and often implicit perspective used in most disciplines of the social and life sciences, we will call the Machine Metaphor Approach (MMA). Use of the scientific method is what separates scientific, from non-scientific claims about the nature of reality. It consists of all philosophical, theoretical, and empirical tools that can be used to systematically evaluate the veracity of such explanatory claims. The repeated application of the scientific method, to study scientific questions, promises to generate valid (accurate) inferences and reliable (precise) facts about a certain explanatory domain. It does not guarantee that any kind of absolute ‘truth’ will be discovered. One factor affecting the perceived veracity of scientific inferences, is the quality of the body of scientific knowledge from which the inferences were deduced, induced or abducted. For example, when a crisis of confidence about the trustworthiness of facts in the scientific record generated by some sub disciplines of psychological science was suggested (Pashler &amp; Wagenmakers, 2012), the immediate consequence was that the veracity of all claims by psychological science was called into question. Rigorous Open Science Less tangible, but not less important for the perceived veracity of scientific knowledge are concepts such as intellectual honesty and scientific integrity of the scientists laying explanatory claims on some domain in reality. Merely checking whether the scientific method has been applied does not fully grasp all the prerequisites for generating a solid body of knowledge. We will use the term rigorous open science to denote the ideal set of conditions that should be in place to allow us to distinguish scientific claims that are likely to be false, from claims that are likely to be true, given the perceived verisimilitude (truth-likeness) of the knowledge accumulated in the scientific record. Figure 1.1: Rigorous Science according to Casadevall &amp; Fang (2016). When a claim is based on Scientific Rigour (Casadevall &amp; Fang, 2016), we mean it was posited based on the following set of principles: Experimental Redundancy - The claim has been examined by all methodological and analytical tools that are available and are appropriate given the context. Rigorous Science does not rely on one type of experimental design or one type of statistical analysis. Recognition of Error - Without failure there can be no progress, therefore we should carefully study failures and not just report success stories. Any sources of error should be carefully studied and reported to the scientific community. Sound Probability &amp; Statistics - Use of the most recent and appropriate statistical theories, models and analytical techniques. Statistical modelling techniques become more realistic over time and often the models that were taught in undergraduate statistics courses have long been replaced and should not be used any more. Efforts to Avoid Logical Traps - When generating theories and defining constructs and laws, make sure logical inconsistencies are avoided. When making inferences, avoid the common logical traps such as The Effect = Structure Fallacy in null hypothesis significance testing (NHST). Intellectual Honesty - Rigorous science is ethical, has integrity and thrives on critical reflection on scientific practice. The right mindset is “Prove yourself wrong!”, not “Prove yourself right!” We add to the list that science must be open and transparent. This may seem like an obvious statement to a fresh student of human behaviour, but concepts that make up an essential part of the scientific debate in 2017, such as open science, open data, reproducibility, Questionable Research Practices (QRPs), Hypothesizing After the Results are Known (HARKing) and preregistration, were practically unknown 5 years ago. Theoretical Tunnelvision “It is the theory that decides what we may observe” —Einstein (as quoted by Heisenberg) Many of the initiatives proposed to improve the social and life sciences focus on improving methodology and statistics. This is understandable, it’s where errors are easily made (and discovered) and it allows for relatively simple interventions, e.g. more stringent control on appropriate use of statistics by journals. However, the goal of generating empirical facts is ultimately because we want to find out which scientific claim about the structure of reality best explains why those empirical facts were observed. The quote attributed to Einstein refers to an important, and grossly underestimated phenomenon one might call the theoretical tunnelvision. It is best explained by an example that is commonly encountered in the literature in psychological science and goes something like this: A study tries to find independent causes (predictors) of a certain disease-entity, a pathological state or behavioural mode people can ‘get stuck in’. Typically, a statistical model fitted on a large, representative sample of individuals in which many different predictors were measured will yield associations between predictor and disease-entity that are significant but small (on average \\(r \\approx 0.3\\), or \\(\\approx 9\\%\\) explained variance). Often, if other known (non-clinical) covariates are included in a model, or, if the multivariate nature of the phenomenon is taken seriously by including repeated measurements and/or multiple dependent variables, these predictors will no longer explain any unique variance in the outcome measures. Here’s an example of a ‘predictor’ study (Walker &amp; Druss, 2015) to find predictors of persistence of Major Depressive Disorder MDD 10 over the course of 10 years in a representative sample of 331 individuals who suffered MDD 10 years earlier: “Clinical variables in this analysis were not strongly associated with persistence of MDD over the course of 10 years. Comorbid generalized anxiety disorder, baseline depression severity, and taking a prescription for nerves, anxiety, or depression were significantly associated with persistent depression in the unadjusted logistic regression models, but the associations became non-significant when in the multivariate model. These findings are in contrast to the results from several other studies.” The study concludes by discussing three factors that play a statistically significant role in the persistence of MDD (text between brackets not in original): “having two or more chronic medical conditions [in 1995-1996] contributes to experiencing depression ten years later. [2.89 more likely] However, only having one chronic medical condition did not increase the odds of being classified as having MDD in 2004–2006.” “days of activity limitation in 1995–1996 were significantly associated with a greater risk of depression ten years later, [2.19 more likely] independent of the number of chronic medical conditions a person had.” “Individuals who were in contact with family less than once a week [in 1995-1996] were more likely to have MDD in 2004–2006. [2.07 more likely] Likewise, people who were married were less likely to have persistent depression compared to those who have never married [never married 2.42 more likely]” So? What’s wrong? So what’s wrong with these inferences? The study shows some previous assumptions about the relevance of clinical predictors should be reconsidered, and it adds to scientific record some facts about risk factors that might have eluded scientists, clinicians and health professionals. Let’s look at the main conclusion of the study, in addition to a plea for more attention for people with two or more chronic medical conditions, Walker &amp; Druss (2015) end the article with: Future research should continue to examine the complex nature of the relationship between chronic medical disorders and comorbid psychiatric conditions. Addressing these conditions and strengthening social support systems could be important strategies for reduce the burden of depression. Here’s what is odd from the perspective of rigorous science: If clinical predictors play no role in explaining why some people remain depressed for such long periods of time, why isn’t the main conclusion of the study that we must re-appraise the scientific theories laying explanatory claim on the aetiology of MDD? It is from these theories that the diagnostic tools, the medical, and psychological interventions to which these patients have been exposed, were derived. Even though the authors acknowledge –and indeed show– that the propagation of a pathological state like MDD over many years is a very complex multivariate phenomenon, their suggestion for future research is still based on an implicit assumption about causation that is extremely simple. The idea is that there is a chain of unique (efficient) causes, each contributing independently to the emergence, and persistence in time of the MDD state. The authors basically suggest some component causes have to be added to the aetiology. The metaphor is that of a machine of which the sum output of its constituent components is equal to the purpose or function of the machine as a whole. Should a component fail, then it can be repaired or replaced as long as it performs the same function as the defective part, thereby restoring the function of the machine as a whole. This is why the authors suggest that strengthening social support systems could be an intervention to reduce the burden of depression: The absence of a partner or visits by family members were predictors that explained some unique variance in the data on the persistence of MDD. Obviously, restoring this defective social support component should restore or at least facilitate the escape from the MDD state. Meanwhile, they seem to forget that they convincingly argued that MDD is a very complex phenomenon that cannot be dissected into neat, independent component causes. Very much related to the previous point: The authors mention three important factors in the discussion and conclusion section, however, the results section contains another factor that was omitted, it is in fact the second most important predictor of the persistence of MDD: “Women had 2.48 the odds of remaining depressed compared to men” Why did they ignore this predictor in the discussion? This is speculation, but could it be that this factor is not mentioned because it would have to be considered a ‘deficient’ component and suggesting any kind of ‘treatment’ intended to ‘repair’ it is of course beyond the realm of sane things to suggest. Nevertheless, it does seem rather important to figure out why women are 2.5 times more likely than men to still be depressed after 10 years. Perhaps not considering gender to be a unique causal component in a chain of independent predictors might help. Instead, gender could be considered a complex aggregate, or, contextual variable that is associated to the dependent variable through a vast network of interdependent facts, events and states of affair. An obvious factor of importance is that effect-studies of medical interventions are mainly conducted on white, male, 20-30 year old, right-handed, subjects with above average SES. Also, it is likely that on average, the stability of mood over longer periods of time is more variable in women than in men due to fluctuations of hormone levels, but also due to antenatal and postnatal depression (World Health Organization, 2002). It does not seem unreasonable to suggest this poses extra challenges for women who want to escape the MDD state. No such thing as theory-free ‘facts’ The analytical tools selected by the researchers (a generalized linear statistical model) restricts the kinds of associations we might observe in the data. In the the present case all associations will –after transformation– be linear compositions of independent components.1 One never reads this valid equally valid conclusion: “We conclude that the linear model is inadequate to describe the complexity of this phenomenon.” The reason is that the implicit assumptions about causality underlying scientific claims never enter the empirical cycle and therefore escape falsification by the repeated application of the scientific method even though those causality assumptions are also based on a scientific theory about the structure of reality that is in principle falsifiable. To be a bit more precise about the relationship between what theories assume to be constituent parts of reality and why, we discuss the differences between some important theoretical concepts. Naturally, if one would use mixed models we can account for dependencies in the data, but they will still be limited to linear associations.↩ "],
["phenomena-theories-facts-and-laws.html", "1.1 Phenomena, theories, facts and laws", " 1.1 Phenomena, theories, facts and laws “All science is either physics or stamp collecting.” —Ernest Rutherford (Physics Nobel Laureate, 1872-1937) It’s important to distinguish between phenomena, hypothesis, theory and law. For example, we will be discussing, nonlinear phenomena, catastrophe theory and power law scaling. The video is provides a very clear explanation of the differences between these concepts. "],
["appraising-and-amending-theories.html", "1.2 Appraising and amending theories", " 1.2 Appraising and amending theories A difficulty of much psychological theorizing is vagueness in the terms employed. In this work, the above ideas have been studied in mathematical form throughout, the definitions and proofs being given corresponding precision. —W. R. Ashby in ‘The Physical Origin of Adaptation by Trial adn Error’ (Ashby (1945), p. 13) Strong Inference The Effect = Structure Fallacy refers to the logical error that occurs a predicted effect is observed (i.e. a statistically significant test result leads to a rejection of the null hypothesis), it is not valid to infer the existence of the assumed cause was evidenced. NHST is based on the falsification principle, which means the perceived veracity of a scientific claim will increase only if it has resisted many rigorous attempts to prove it is wrong. If a scientific claim has a large track-record of resisting falsification Table 1.1: Strong Inference according to Platt (1964) Strong inference consists of applying the following steps to every problem in science, formally and explicitly and regularly: Devising alternative hypotheses Devising a crucial experiment (or several of them), with alternative possible outcomes, each of which will, as nearly as possible, exclude one or more of the hypotheses Carrying out the experiment so as to get a clean result 1’ Recycling the procedure, making subhypotheses or sequential hypotheses to refine the possibilities that remain … and so on. –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; "],
["study-materials.html", "Study Materials", " Study Materials Ontology. Epistemology "],
["introduction-to-complexity-science.html", "Chapter 2 Introduction to Complexity Science", " Chapter 2 Introduction to Complexity Science Psychological systems are biological systems which are physical systems that are alive. Therefore, any theory that lays explanatory claim to phenomena of the mind, ultimately must be a theory about how a physical system is able to accumulate non-random order into its internal structure that appears to codetermine its behaviour. Less formally stated, a science that studies the behaviour of physical systems that are alive, that appear to have a memory which makes their behaviour adaptive, future oriented and intelligent, should be grounded in physical and biological principles and laws. For now, that may be a bridge too far (however, see Turvey &amp; Carello, 2012), but current theories of human behaviour should at least not contradict highly corroborated theories of physics that also describe the behaviour of the constituent components of complex living systems. "],
["some-notes-on-using-r.html", "Chapter 3 Some Notes on Using R", " Chapter 3 Some Notes on Using R You have probably heard many people say they should invest more time and effort to learn to use the R software environment for statistical computing… and they were right. However, what they probably meant to say is: “I tried it, but it’s so damned complicated, I gave up”… and they were right. That is, they were right to note that this is not a point and click tool designed to accommodate any user. It was built for the niche market of scientists who use statistics, but in that segment it’s actually the most useful tool I have encountered so far. "],
["new-to-r.html", "3.1 New to R?", " 3.1 New to R? Now that your struggles with getting a grip on R are fully acknowledged in advance, let’s try to avoid the ‘giving up’ from happening. Try to follow these steps to get started: Get R and add some user comfort: Install the latest R software and install a user interface like RStudio… It’s all free! An R interface will make some things easier, e.g., searching and installing packages from repositories. R Studio will also add functionality, like git/svn version control, project management and more, like the tools to create html pages like this one (knitr and Rmarkdown). Another source of user comfort are the packages. R comes with some basic packages installed, but you’ll soon need to fit generalised linear mixture models, or visualise social networks using graph theory and that means you’ll be searching for packages that allow you to do such things. A good place to start package hunting are the CRAN task view pages. Learn by running example code: Copy the commands in the code blocks you find on this page, or any other tutorial or help files (e.g., Rob Kabacoff’s Quick R). Paste them into an .R script file in the script (or, source) editor. In R Studio You can run code by pressing cmd + enter when the cursor is on a single single line, or you can run multiple lines at once by selecting them first. If you get stuck remember that there are expert R users who probably have answered your question already when it was posted on a forum. Search for example through the Stack overflow site for questions tagged with R) Examine what happens… when you tell R to make something happen: R stores variables (anything from numeric data to functions) in an Environment. There are in fact many different environments, but we’ll focus on the main workspace for the current R session. If you run the command x &lt;- 1+1, a variable x will appear in the Environment with the value 2 assigned to it. Examining what happens in the Environment is not the same as examining the output of a statistical analysis. Output in R will appear in the Console window. Note that in a basic set-up each new R session starts with an empty Environment. If you need data in another session, you can save the entire Environment, or just some selected variables, to a file (.RData). Learn about the properties of R objects: Think of objects as containers designed for specific content. One way to characterize the different objects in R is by how picky they are about the content you can assign it. There are objects that hold character and numeric type data, a matrix for numeric data organised in rows and columns, a data.frame is a matrix that allows different data types in columns, and least picky of all is the list object. It can carry any other object, you can have a list of which item 1 is an entire data.frame and item 2 is just a character vector of the letter R. The most difficult thing to master is how to efficiently work with these objects, how to assign values and query contents. Avoid repeating yourself: The R language has some amazing properties that allow execution of many repetitive algorithmic operations using just a few lines of code at speeds up to warp 10. Naturally, you’ll need to be at least half Vulcan to master these features properly and I catch myself copying code when I shouldn’t on a daily basis. The first thing you will struggle with are the apply functions. These functions pass the contents of a list object to a function. Suppose we need to calculate the means of column variables in 40 different SPSS .sav files stored in the folder DAT. With the foreign package loaded we can execute the following commands: data &lt;- lapply(dir(&quot;/DAT/&quot;,pattern=&quot;.sav$&quot;),read.spss) out &lt;- sapply(data,colMeans) The first command applies read.spss to all files with a .sav extension found in the folder /DAT. It creates a data frame for each file which are all stored as elements of the list data. The second line applies the function colMeans to each element of data and puts the combined results in a matrix with dataset ID as columns (1-40), dataset variables as rows and the calculated column means as cells. This is just the beginning of the R magic, wait ’till you learn how to write functions that can create functions. "],
["tutorials.html", "3.2 Getting started with R tutorials", " 3.2 Getting started with R tutorials Tutorials on using functions: Quick-R Software Carpentry Nicer Code Advanced R Tutorials on using conditionals and for loops: Quick-R Software Carpentry R-Bloggers Tutorials on the -ply family of functions: R-bloggers Nicer Code R for Dummies Plotting, plotting and more plotting: A Compendium of Clean Graphs in R ggplot2 reference ggplot2 extensions patchwork, the ultimate ggplot2 extension The R-graph gallery Quick-R Nicer Code Tutorial on Effect Size Confidence Intervals and more: In this tutorial on estimating Effect Size Confidence Intervals (ESCI) there are a lot of examples on how to use R. It was written as an addendum for a post on the Open Science Collaboration Blog, which contains many interesting entries on diverse subjects (like behavioural priming, theoretical amnesia and anonymous peer review) "],
["not-new-to-r.html", "3.3 Not new to R?", " 3.3 Not new to R? If you have been using R for a while, but do not consider yourself a master yet, I recommend learning to use the tidyverse packages and the accompanying web-book R for data scientists. Welcome to the tidyverse: Install the tidyverse Learn how to use the tidyverse Learn how to use the tidyverse to do statistics Learn how to use the tidyverse to create networks How to make R purrr "],
["introduction-to-the-mathematics-of-change.html", "Chapter 4 Introduction to the Mathematics of Change", " Chapter 4 Introduction to the Mathematics of Change The simplest non-trivial iterative change process can be described by the following difference equation: \\[ Y_{t+1} = Y_{t=0} + a*Y_t \\] The equation describes the way in which the value of \\(Y\\) changes between two adjacent, discrete moments in time (hence the term difference equation, or recurrence relation). There are two parameters resembling an intercept and a slope: The starting value \\(Y_0\\) at \\(t=0\\), also called the starting value, or the initial conditions. A rule for incrementing time, here the change in \\(Y\\) takes place over a discrete time step of 1: \\(t+1\\). The values taken on by variable \\(Y\\) are considered to represent the states quantifiable observable alternative ways to describe the change of states : A dynamical rule describing the propagation of the states of a system observable measured by the values of variable Y through discrete time. A dynamic law describing the time-evolution of the states of a system observable measured by the variable Y. These descriptions all refer to the change processes that govern system observables (properties of dynamical systems that can be observed through measurement). "],
["its-a-line-its-a-plane.html", "4.1 It’s a line! It’s a plane!", " 4.1 It’s a line! It’s a plane! The formula resembles the equation of a line. There is a constant value \\(Y_{0}\\) which is added to a proportion of the value of \\(Y\\) at time \\(t\\), given by parameter \\(a\\). This is equivalent to the slope of a line. However, in a \\((X,Y)\\) plane there are two ‘spatial’ (metric) dimensions representing the values two variables \\(X\\) and \\(Y\\) can take on (see figure). The best fitting straight line would be called a statistical model of the linear relationship between the observed values of \\(X\\) and \\(Y\\). It can be obtained by fitting a General Linear Model (GLM) to the data. If \\(X\\) were to represent repeated measurements the multivariate GLM for repeated measures would have to be fitted to the data. This can be very problematic, because statistical models rely on Ergodic theory: “… it is the study of the long term average behavior of systems evolving in time.” In other words: If you throw 1 die 100 times in a row, the average of the 100 numbers is the time-average of one of the observables of die-throwing systems. If this system is ergodic, then its time-average is expected to be similar to the average of the numbers that turn up if you throw 100 dice all at the same instance of time. The dice layed out on the table represent a spatial sample, a snapshot frozen in time, of the possible states the system can be in. Taking the average would be the spatial average this observable of die-throwing systems. This ergodic condiciotn is often implicitly assumed in Behavioural Science when studies claim to study change by taking different samples of individuals (snapshots of system states) and comparing if they are the same. need to assume independence of measurements within and between subjects. These assumptions can be translated to certain conditions that must hold for the model to be valid, known as Compound Symmetry and Sphericity: The compound symmetry assumption requires that the variances (pooled within-group) and covariances (across subjects) of the different repeated measures are homogeneous (identical). This is a sufficient condition for the univariate F test for repeated measures to be valid (i.e., for the reported F values to actually follow the F distribution). However, it is not a necessary condition. The sphericity assumption is a necessary and sufficient condition for the F test to be valid; it states that the within-subject “model” consists of independent (orthogonal) components. The nature of these assumptions, and the effects of violations are usually not well-described in ANOVA textbooks;2 As you can read in the quoted text above, these conditions must hold in order to be able to identify unique independent components as the sources of variation of \\(Y\\) over time within a subject. This is the a clear example of: If you choose to use GLM repeated measures to model change over time, you will only be able to infer independent components that are responsible for the time-evolution of \\(Y\\). As is hinted in the last sentence of the quote, the validity of such inferences is not a common topic of discussion statistics textbooks. Retreived from www.statsoft.com↩ "],
["no-its-a-time-series.html", "4.2 No! … It’s a time series!", " 4.2 No! … It’s a time series! The important difference between a regular 2-dimensional Euclidean plane and the space in which we model change processes is that the \\(X\\)-axis represents the physical dimension time. In the case of the Linear Map we have a 1D space with one ‘spatial’ dimension \\(Y\\) and a time dimension \\(t\\). This is called time series if \\(Y\\) is sampled as a continuous process, or a trial series if the time between subsequent observations is not relevant, just the fact that there was a temporal order (for example, a series of response latencies to trials in a psychological experiment in the order in which they were presented to the subject). Time behaves different from a spatial dimension in that it is directional (time cannot be reversed), it cannot take on negative values, and, unless one is dealing with a truly random process, there will be a temporal correlation across one or more values of \\(Y\\) separated by an amount of time. In the linear difference equation this occurs because each value one step in the future is calculated based on the current value. If the values of \\(Y\\) represent an observable of a dynamical system, the system can be said to have a history, or a memory. Ergodic systems do not have a history or a memory that extends across more than one time step. This is very convenient, because one can calculate the expected value of a system observable given infinite time, by making use of of the laws of probabilities of random events (or random fields). This means: The average of an observable of an Ergodic system measured across infinite time (its entire history, the time-average), will be the be the same value as the average of this observable measured at one instance in time, but in an infinite amount of systems of the same kind (the population, the spatial average) [^dice]. The simple linear difference equation will have a form of perfect memory across the smallest time scale (i.e., the increment of 1, \\(t+1\\)). This ‘memory’ just concerns a correlation of 1 between values at adjacent time points (a short range temporal correlation, SRC), because the change from \\(Y_t\\) to \\(Y_{t+1}\\) is exactly equal to \\(a * Y_t\\) at each iteration step. This is the meaning of deterministic, not that each value of \\(Y\\) is the same, but that the value of \\(Y\\) now can be perfectly explained form the value of \\(Y\\) one moment in the past. Summarising, the most profound difference is not the fact that the equation of linear change is a deterministic model and the GLM is a probabilistic model with parameters fitted from data, this is something we can (and will) do for \\(a\\) as well. The profound difference between the models is the role given to the passage of time: The linear difference equation represents changes in \\(Y\\) as a function of the physical dimension time and \\(Y\\) itself. The GLM represents changes in \\(Y\\) as a function of a linear predictor composed of additive components that can be regarded as independent sources of variation that sum up to the observed values of \\(Y\\). "],
["plotTS.html", "4.3 R: The time series object", " 4.3 R: The time series object A time series object is expected to have a time-dimension on the x-axis. This is very convenient, because R will generate the time axis for you by looking at the time series properties attribute of the object. Even though we are not working with measurement outcomes, consider a value at a time-index in a time series object a sample: Start - The value of time at the first sample in the series (e.g., \\(0\\), or \\(1905\\)) End - The value of time at the last sample in the series (e.g., \\(100\\), or \\(2005\\)) Frequency - The amount of time that passed between two samples, or, the sample rate (e.g., \\(0.5\\), or \\(10\\)) Examples of using the time series object. # Get a timeseries of 100 random numbers Y &lt;- ts(rnorm(100)) # plot.ts plot(Y) # Get sample rate info tsp(Y) ## [1] 1 100 1 # Extract the time vector time(Y) ## Time Series: ## Start = 1 ## End = 100 ## Frequency = 1 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 For now, these values are in principle all arbitrary units (a.u.). These settings only make sense if they represent the parameters of an actual measurement procedure. It is easy to adjust the time vector, by assigning new values using tsp() (values have to be possible given the time series length). For example, suppose the sampling frequency was \\(0.1\\) instead of \\(1\\) and the Start time was \\(10\\) and End time was \\(1000\\). # Assign new values (tsp(Y) &lt;- c(10, 1000, .1)) ## [1] 1e+01 1e+03 1e-01 # Time axis is automatically adjusted time(Y) ## Time Series: ## Start = 10 ## End = 1000 ## Frequency = 0.1 ## [1] 10 20 30 40 50 60 70 80 90 100 110 120 130 140 ## [15] 150 160 170 180 190 200 210 220 230 240 250 260 270 280 ## [29] 290 300 310 320 330 340 350 360 370 380 390 400 410 420 ## [43] 430 440 450 460 470 480 490 500 510 520 530 540 550 560 ## [57] 570 580 590 600 610 620 630 640 650 660 670 680 690 700 ## [71] 710 720 730 740 750 760 770 780 790 800 810 820 830 840 ## [85] 850 860 870 880 890 900 910 920 930 940 950 960 970 980 ## [99] 990 1000 4.3.1 Plotting a ts object as a time series Depending on which packages you use, there will be different settings applied to time series objects created by ts(). Below are some examples of differences between plotting routines. require(lattice) # Needed for plotting require(latticeExtra) # Needed for plotting # stats::plot.ts plot(Y, lwd = 2, main = &quot;stats::plot.ts&quot;) # lattice::xyplot.ts xyplot(Y, lwd = 2, main = &quot;lattice::xyplot.ts&quot;) 4.3.2 Plotting multiple time series in one figure Plot multiple time series in frames with plot.ts() in package::stats. This function takes a matrix as input, here we use cbind( ... ). # stats::plot.ts plot(cbind(Y, cumsum(Y), cumsum(c(0,diff(Y))) ), yax.flip = TRUE, col = &quot;blue&quot;, frame.plot = TRUE) title(main = expression(paste(&quot;Random Numbers: &quot;,N(0,sigma))), xlab = &quot;time (a.u.)&quot;) Plot multiple time series in one graph with ts.plot() in package::graphics. This function can handle multiple ts objects as arguments. # graphics::ts.plot ts.plot(Y, cumsum(Y), cumsum(c(0,diff(Y))), gpars = list(xlab = &quot;time (a.u.)&quot;, ylab = expression(Y(t)), main = expression(paste(&quot;Random Numbers: &quot;,N(0,sigma))), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;) ) ) legend(0, -10, c(&quot;Y&quot;,&quot;cumsum(Y)&quot;, &quot;cumprod(Y)&quot;), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;), merge = TRUE, cex=.9) Use xyplot() in package::lattice to create a plot with panels. The easiest way to do this is to create a dataset in so-called “long” format. This means the variable to plot is in 1 column and other variables indicate different levels, or conditions under which the variable was observed or simulated. Function ldply() is used to generate \\(Y\\) for three different settings of \\(r\\). The values of \\(r\\) are passed as a list and after a function is applied the result is returned as a dataframe. require(plyr) # Needed for function ldply() # Create a long format dataframe for various values for `r` data &lt;- cbind.data.frame(Y = c(as.numeric(Y), cumsum(Y), cumsum(c(0,diff(Y)))), time = c(time(Y), time(Y), time(Y)), label = factor(c(rep(&quot;Y&quot;,length(Y)), rep(&quot;cumsum(Y)&quot;,length(Y)), rep(&quot;cumsum(diff(Y))&quot;,length(Y)))) ) # Plot using the formula interface xyplot(Y ~ time | label, data = data, type = &quot;l&quot;, main = expression(paste(&quot;Random Numbers: &quot;,N(0,sigma)))) 4.3.3 The return plot To create a return plot the values of \\(Y\\) have to be shifted by a certain lag. The functions lead() and lag() in package::dplyr are excellent for this purpose (note that dplyr::lag() behaves different from stats::lag()). # Function lag() and lead() require(dplyr) # Get exponential growth Y &lt;- rnorm(1000) Y1 &lt;- Y/max(Y) # Get logistic growth in the chaotic regime Y2 &lt;- cumsum(c(0,diff(Y)))/max(cumsum(c(0,diff(Y)))) # Use the `lag` function from package `dplyr` op &lt;- par(mfrow = c(1,2), pty = &quot;s&quot;) plot(dplyr::lag(Y1), Y1, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = &quot;Y(t)&quot;, ylab = &quot;Y(t+1)&quot;, main = &quot;rnorm(1000) / max&quot;) plot(dplyr::lag(Y2), Y2, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = &quot;Y(t)&quot;, ylab = &quot;Y(t+1)&quot;, main = &quot;cumsum(diff(Y)) / max&quot;) par(op) Use l_ply() from package::plyr to create return plots with different lags. The l_ before ply means the function will take a list as input to a function, but it will not expect any data to be returned, for example in the case of a function that is used to plot something. # Explore different lags op &lt;- par(mfrow = c(1,2), pty = &quot;s&quot;) plyr::l_ply(1:4, function(l) plot(dplyr::lag(Y2, n = l), Y2, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = &quot;Y(t)&quot;, ylab = paste0(&quot;Y(t+&quot;,l,&quot;)&quot;), cex = .8)) par(op) 4.3.4 Using ggplot2 Becoming proficient at ggplot2 can take some time, but it does pay off. One of the problems with plotting time series data is that ggplot2 wants tidy data in long format. Tidy data is: Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: 1. Each variable forms a column. 2. Each observation forms a row. 3. Each type of observational unit forms a table. —Wickham &amp; others (2014) "],
["implementing-iterative-functions.html", "4.4 Implementing iterative functions", " 4.4 Implementing iterative functions Coding change processes (difference equations) in Matlab and R is always easier than using a spreadsheet. One obvious way to do it is to use a counter variable representing the iterations of time in a for ... next loop (see tutorials). The iterations should run over a vector (which is the same concept as a row or a column in a spreadsheet: An indexed array of numbers or characters). The first entry should be the starting value, so the vector index \\(1\\) represents \\(Y_0\\). The loop can be implemented a number of ways, for example as a function which can be called from a script or the command or console window. In R working with functions is easy, and very much recommended (see tutorials), because it will speed up calculations considerably, and it will reduce the amount of code you need to write. You need to gain some experience with coding in R before you’ll get it right. In order to get it lean and clean (and possibly even mean as well) you’ll need a lot of experience with coding in R,therefore, we will (eventually) provide you the functions you’ll need to complete the assignments in the Answers section of the assignments. If you get stuck, look at the answers. If you need to do something that reminds you of an assignment, figure out how to modify the answers to suit your specific needs. "],
["numerical-integration-to-simulate-continuous-time.html", "4.5 Numerical integration to simulate continuous time", " 4.5 Numerical integration to simulate continuous time In order to ‘solve’ a differential equation for continuous time using a method of numerical integration, one could code it like in the spreadsheet assignment below. For R and Matlab there are so-called solvers available, functions that will do the integration for you. For R look at the Examples in package deSolve. Euler’s method and more… The result of applying a method of numerical integration is called a numerical solution of the differential equation. The analytical solution is the equation which will give you a value of \\(Y\\) for any point in time, given an initial value \\(Y_0\\). Systems which have an analytical solution can be used to test the accuracy of numerical solutions. Analytical solution Remember that the analytical solution for the logistic equation is: \\[ Y(t) = \\frac{K}{1 + \\left(\\frac{K}{Y_{0} - 1}\\right) * e^{-r*t} } \\] If we want to know the growth level \\(Y_t\\) at \\(t=10\\), with \\(Y_0=.0001\\), \\(r=1.1\\) and \\(K=4\\), we can just fill it in: # Define a function for the solution logSol &lt;- function(Y0, r, K, t){K/(1+(K/Y0-1)*exp(-r*t))} # Call the function logSol(Y0=.0001, r=1.1, K=4, t=10) ## [1] 2.398008 We can pas a vector of time points to create the exact solution, the same we would get if we were to iterate the differential/difference equation. # Plot from t=1 to t=100 plot(logSol(Y0=.0001, r=1.1, K=4, t=seq(1,20)), type = &quot;b&quot;, ylab = expression(Y[t]), xlab = &quot;t&quot;) # Plot t=10 in red points(10,logSol(Y0=.0001, r=1.1, K=4, t=10), col=&quot;red&quot;, pch=16) Numerical solution (discrete) If we would iterate the differential equation … \\[ \\frac{dY}{dt} = Y_t * (1 + r - r * \\frac{Y_t}{K}) \\] … as if it were a difference equation, that is, not simulating continuous time. logIter &lt;- function(Y0,r,K,t){ N &lt;- length(t) Y &lt;- as.numeric(c(Y0, rep(NA,N-2))) sapply(seq_along(Y), function(t){ Y[[t+1]] &lt;&lt;- Y[t] * (1 + r - r * Y[t] / K)}) } # Plot from t=1 to t=100 plot(logIter(Y0=.0001, r=1.1, K=4, t=seq(1,20)), type = &quot;b&quot;, ylab = expression(Y[t]), xlab = &quot;t&quot;) # Plot t=10 in red points(10,logSol(Y0=.0001, r=1.1, K=4, t=10), col=&quot;red&quot;, pch=16) "],
["advmodels.html", "4.6 Modeling interactions between processes and agents", " 4.6 Modeling interactions between processes and agents 4.6.1 The Competetive Lottka-Volterra Equations The coupled predator-prey dynamics in the previous assignment are not a very realistic model of an actual ecological system. Both equations are exponential growth functions, but Rabbits for example, also have to eat! One way to increase realism is to consider coupled logistic growth by introducing a carrying capacity. Follow the link to the Wiki page and try to model the system! This is what interaction dynamics refers to, modeling mutual dependiencies using the if ... then conditional rules isn’t really about interaction, or coupling between processes. 4.6.2 Predator-Prey (and other) dynamics as Agent Based Models Agent-Based models are an expansion of the idea of “connected growers” that includes a spatial location of the things that is subject to change over time. Have a look at some of the NETlogo demo’s: Rabbits Weeds Grass Wolf Sheep Grass "],
["basic-tsa.html", "Chapter 5 Basic TSA", " Chapter 5 Basic TSA "],
["applications.html", "Chapter 6 Applications", " Chapter 6 Applications Some significant applications are demonstrated in this chapter. "],
["example-one.html", "6.1 Example one", " 6.1 Example one "],
["example-two.html", "6.2 Example two", " 6.2 Example two "],
["final-words.html", "Chapter 7 Final Words", " Chapter 7 Final Words We have finished a nice book. "],
["final-words-1.html", "Chapter 8 Final Words", " Chapter 8 Final Words We have finished a nice book. "],
["final-words-2.html", "Chapter 9 Final Words", " Chapter 9 Final Words We have finished a nice book. "],
["final-words-3.html", "Chapter 10 Final Words", " Chapter 10 Final Words We have finished a nice book. "],
["list-of-terms.html", "A List of terms", " A List of terms Adaptive Behaviour System behaviour that appears to be (partially) coordinated by previously ‘experienced events’ Analytic solution The solution to a difference or differential equation allows one to find any state of the system without the need to iterate the model starting from some initial condition. There are very few (systems of) equations for which analytical solutions exist Attractor The status that a dynamic system eventually “settles down to”. An attractor is a set of values in the phase space to which a system migrates over time, or iterations. Attractors can have as many dimensions as the number of variables that influence its system Basin of attraction A region in phase space associated with a given attractor. The basin of attraction of an attractor is the set of all (initial) points that eventually end up in that attractor Behaviour (of a dynamic system) The temporal evolution of states of a system according to one or more rules (also known as state propagation rules, or, iterative processes). Models of the behaviour of dynamic systems use difference or differential equations to describe the iterative processes hypothesized to underlie the temporal evolution Bifurcation A clearly observable qualitative change in the behavioural mode (attractor state) of a dynamic system associated with continuous change in one or more control parameters (also known as Phase-, State-, or Order- Transition). The value of a control parameter at which a bifurcation occurs is often non-specific, or trivial Bifurcation diagram Visual summary of the succession of period-doubling bifurcations produced by gradual changes in the control parameter(s) Catastrophe flags Markers indicative for a physical system that is described by a catastrophe. There are 5 ‘classical’ flags and 3 ‘diagnostic’ flags. Classical: bimodality, sudden jumps, inaccessibility, sensitivity &amp; hysteresis. Diagnostic: divergence from linear response, critical slowing down and critical fluctuations. Diagnostic flags can be used as early-warning signals. Catastrophe theory Mathematical research program describing how gradual change in some parameters can lead to disproportionately large changes in another parameter, called catastrophes (similar to bifurcations, Phase-, State-, or Order-transitions). ‘This kind of behaviour has been summarized succinctly in the phrase “the straw that broke the camel’s back”.’ (Gilmore, 1992). Complex Network A network with of many nodes and likely many substructures depending on that nature and distribution of connections between nodes Complex system Spatially and/or temporally extended nonlinear systems characterized by emergent properties and self-organised behavioural modes at a global, or, macro-level (the system as a whole), that is often different from the characteristic behaviour at a local, or, micro-level (behaviour of the individual parts that constitute the whole) Complexity science Complexity science studies how systems that consist of many components can generate relatively simple and stable (non-random) behaviour. Important behavioural phenomena studied in Complexity Science are synchronisation, adaptation and coordination of behaviour across many different temporal and spatial scales, emergent properties and collective behaviour, holism and self-organisation Component dominant dynamics A causal ontology in which observed behaviour is explained by assuming it is the result of a chain of independent efficient causes (components) Control parameter A variable that controls the global behaviour of a dynamic system. For certain values of the parameter, transitions between qualitatively different behavioural modes (orders) can occur. Critical fluctuations An early warning signal for a phase transition that is characterised by an increase in fluctuations (variability) of the behaviour of the system. The increase occurs because the self-organised transition from one state to another relaxes the constraints on the degrees of freedom a system has available to generate its behaviour, allowing states and behavioural modes to appear that were previously inaccessible. Critical slowing down An early warning signal for a phase transition that is characterised by an increase in the duration of relaxation times. If it takes longer for the system to return to the state it was perturbed from, this implies the emergence of a new stable state is imminent Deterministic Chaos Behaviour of a dynamic system that “looks random, but is not” (Lorenz, 1973). The dynamics can be characterised as follows: 1) A-periodic, no point or trajectory in state space will exactly recur; 2) Sensitive dependence in initial conditions; 3) Bounded, not all theoretically possible degrees of freedom are available to the system; 4) The origin of this behaviour is deterministic, not stochastic Difference equation A function specifying the underlying change process in a variable from one discrete point in time to another Differential equation A function specifying the underlying change process of a variable in continuous time Dimension See embedding dimension, box-counting dimension, correlation dimension, information dimension, dimension of a system Dimensions of a system The set of variables that define a system. Iterative processes operate on the dimensions of a system Dynamic system A set of equations specifying how certain variables change over time. The equations specify how to determine (compute) the new values as a function of their current values and control parameters. The functions, when explicit, are either difference equations or differential equations. Dynamic systems may be stochastic or deterministic. In a stochastic system, new values come from a probability distribution. In a deterministic system, a single new value is associated with any current value Early warning signals Critical slowing down and critical fluctuations. Early-warning signals indicate instability in the existing state which may result in a qualitative shift towards a new state (phase transition / catastrophe). Early-warning signals are similar to diagnostic catastrophe flags. Effective Complexity “The effective complexity of an entity is the length of a highly compressed description of its regularities.” (Gell-man &amp; Lloyd, 2004) Embedding Dimension Successive N-tuples of points in a time series are treated as points in N dimensional space. The points are said to reside in embedding dimensions of size N, for N = 1, 2, 3, 4, … etc. Emergence A complex system can generate emergent behaviour or display emergent properties that are novel and unexpected, that is, they are not predictable from the behaviour and properties of the components of the system Entropy Relative absence of order/redundancy in a system. The degrees of freedom a system has available for generating its behaviour: Possibility Epigenetic landscape (potential landscape) A hypothetical landscape describing the relative stability of behavioural modes of a system over time Experienced event An interaction of a system with its environment that changed the internal structure/organization of the system such that it can be said to display adaptive behaviour. “Interaction with after-effects”. Random behaviour is “Interaction without after-effects”. flow ~ A differential equation Fractal An irregular shape with self-similarity. It has infinite detail, and cannot be differentiated. “Wherever chaos, turbulence, and disorder are found, fractal geometry is at play” (Briggs and Peat, 1989). Fractal Dimension A measure of a geometric object that can take on fractional values. At first used as a synonym to Hausdorff dimension, fractal dimension is currently used as a more general term for a measure of how fast length, area, or volume increases with decrease in scale. (Peitgen, Jurgens, &amp; Saupe, 1992a). Graph theory Models in which associations between mathematical objects are defined as edges (connections) between vertices (nodes) Hausdorff Dimension A measure of a geometric object that can take on fractional values. (see fractal dimension). Holism (epistemic) “some property of a whole would be holistic if, according to the theory in question, there is no way we can find out about it using only local means, i.e., by using only all possible non-holistic resources available to an agent.” (Seevinck, 2002) Idiographic approach Scientific explanation in which the goal is to generate knowledge about specific facts, events or entities. The goal is not to generalize to universal laws and first principles. Information (quantity) A measurable quantity that resolves uncertainty about the state of a system by assigning a value to the uncertainty. Initial condition The starting point of a dynamic system, the initial state of a system from which it evolved to the current state. Interaction dominant dynamics A causal ontology in which observed behaviour is explained by assuming it is the result of interactions between processes across many temporal and spatial scales Iteration The repeated application of a function, using its output from one application as its input for the next. Iterative function A function used to calculate the new state of a dynamic system. Iterative system A system in which one or more functions are iterated to define the system. Largest Lyapunov exponent The value of the largest exponent in a spectrum of exponents (the Lyapunov spectrum), coefficients of time, that reflect the rate of departure (divergence) of dynamic orbits of a system. The largest exponent indicates the extent to which the behaviour of a system is sensitive to initial conditions. Limit cycle An attractor that is periodic in time, that is, that cycles periodically through an ordered sequence of states. Limit points Points in phase space. There are three kinds: attractors, repellors, and saddle points. A system moves away from repellors and towards attractors. A saddle point is both an attractor and a repellor, it attracts a system in certain regions, and repels the system to other regions. Linear function of predictors A linear equation is of predictors is of the form y=a*x(i)+b, in which variable y varies ‘linearly’ with other variables x(i). In this equation, ‘a’ determines the slope of the line and ‘b’ reflects the y-intercept, the value y obtains when all x(i) equal zero. Linear function of time A linear function of time is of the form ŷ(t) = a*y(t) + b, in which variable y varies ‘linearly’ with time ‘t’, that is, with itself at an earlier moment in time. In this equation ‘a’ determines the rate with which ‘y’ will change as time passes, ‘b’ reflects the initial condition, the value y obtains when t equals zero. map … A difference equation Nonlinear dynamics The study of dynamic systems whose functions specify that change is not a linear function of time. Orbit (trajectory) A sequence of coordinates (a path) through the phase space of a system. Order “order is essentially the arrival of redundancy in a system, a reduction of possibilities”(Von Förster, 2003). Any form of non-random association or dependency that exists between parts of a system, its behaviour over time and/or its environment is a form of order. In scientific explanation of behaviour, the presence of order in non-artificial systems must be explained and should not be (implicitly) assumed. Order Parameter A nominal variable that indexes qualitatively different behavioural modes of a system, for example the phases of matter (gas, liquid, solid, plasma) Period-doubling The change in dynamics in which a N-point attractor is replaced by a 2N-point attractor. Phase portrait The collection of all trajectories from all possible starting points in the phase space of a dynamic system. Phase space An abstract space used to represent the behaviour of a system. Its dimensions are the variables of the system. Thus a point in the phase space defines a potential state of the system. The points actually achieved by a system depend on its iterative function and initial condition (starting point). Phase transition A transition between qualitatively different behavioural modes Potential function A function that describes the order parameter of a system, that is, it describes the relative stability of the potential end-states (attractor states) a system can settle into. The parameters of the potential function include the control parameter. Power-law scaling A relationship between two variables that is linear on doubly logarithmic coordinates, meaning the law is expressed in increments that represent ‘power’ Recursive process For our purposes, “recursive” and “iterative” are synonyms. Thus recursive processes are iterative processes, and recursive functions are iterative functions. Relaxation time The time it takes for a system to return to a stable state after it was perturbed enough to leave that state. A characteristic warning signal of an imminent phase transition is an increase relaxation times, also known as critical slowing down. Repellors One type of limit point. A point in phase space that a system moves away from. Return map Plot of time series values vs. a delayed copy of itself. A return plot can be used to get an idea about the functional form of the iterative process, it is a simple variant of delay embedding. Saddle point A point, usually in three dimensional state space, that both attracts and repels, attracting in one dimension and repelling to another. Scale free network A network in which the distribution of the number of connections of a node and their frequency of occurrence follows a power-law in which there are just a few nodes with many connections and many nodes with just a few connections Self-affinity An infinite nesting of characteristic structure on all scales. Strict self-affinity refers to a form of which all substructures are affine transformation, which means the different dimensions of the system can be scaled by their own exponent. Statistical self-affinity refers to an approximate equivalence of form at all scales. Self-similarity An infinite nesting of characteristic structure on all scales. Strict self-similarity refers to a form of which all substructures can be considered scaling transformations, larger or smaller copies scaled by a single exponent for all dimensions of the structure. Statistical self-similarity refers to an approximate equivalence of scaled structure. Sensitive dependence on initial conditions A property of chaotic systems. A dynamic system has sensitivity to initial conditions when very small differences in starting values result in very different behaviour. If the orbits of nearby starting points diverge, the system has sensitivity to initial conditions. Small world network Many real-world networks have a small average shortest path length, but also a clustering coefficient that is significantly higher than expected by chance. These networks are extremely efficient, each node in a very large network can still be reach in just a few steps (the ’six degrees of separation` phenomenon). State A coordinate in state space designating the current status of a dynamic system. The elements of the coordinates are values on the dimensions of the system that span the state space. State space A hypothetical space spanned by the dimensions of the system. Each combination of values of variables that represent the dimension is a state of the system, it is a coordinate in state space. State space (phase space) An abstract space used to represent the behaviour of a system. Its dimensions are the variables of the system. Thus a point in the phase space defines a potential state of the system. Strange attractor An attractor state representing chaotic dynamics: a-periodic, bounded, and sensitive dependence on initial conditions System An entity that can be described as a composition of components according to some organising principle. Organising principles describe how parts of the system relate to the whole and. Time series A record of observations (data points) of behaviour over time. Trajectory (orbit) A sequence of positions (path) of a system in its phase space. The path from its starting point (initial condition) to and within its attractor. Transient time (transient behaviour) The time it takes for a system to transition from one stable state (behavioural mode, attractor state) into another, during which the system displays transient behaviour "],
["references.html", "References", " References "]
]
