# (PART) Mathematics of Change {-} 

# **Introduction to the Mathematics of Change** 




## **Stat(ist)ic versus Dynamic Models**

The simplest non-trivial *iterative change process* can be described by the following *difference equation*: 

$$ Y_{t+1} = Y_{t=0} + a*Y_t $$    

The equation describes the way in which the value of $Y$ changes [between two adjacent, **discrete** moments in time](https://en.wikipedia.org/wiki/Discrete_time_and_continuous_time) 
(hence the term [difference equation, or recurrence relation](https://en.wikipedia.org/wiki/Recurrence_relation)). There are two parameters resembling an intercept and a slope:

1. The starting value $Y_0$ at $t=0$, also called the *starting value*, or the *initial conditions*.
2. A rule for incrementing time, here the change in $Y$ takes place over a discrete time step of 1: $t+1$.    
    
The values taken on by variable $Y$ are considered to represent the states  quantifiable observable  leAlternative ways to describe the change of states :

* A dynamical rule describing the propagation of the states of a system observable measured by the values of variable `Y` through discrete time.
* A dynamic law describing the time-evolution of the states of a system observable measured by the variable `Y`.   
    
These descriptions all refer to the change processes that govern system observables (properties of dynamical systems that can be observed through measurement).     

### **It's a line! It's a plane!** 
The formula resembles the equation of a line. There is a constant value $Y_{0}$ which is added to a proportion of the value of $Y$ at time $t$, given by parameter $a$. This is equivalent to the slope of a line. However, in a $(X,Y)$ plane there are two 'spatial' (metric) dimensions representing the values two variables $X$ and $Y$ can take on (see figure).

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(12346)
y1 = cumsum(rnorm(n=21,mean=5,sd=100))
x1 = -10:10
plot(x1,y1, type="p", lwd=2, xlim=c(-12,12), yaxt="n", xlab="X", ylab="Y", main = "2D Euclidean Space")
abline(h=0,v=0,lty=2)
l1 <- lm(y1~x1)
lines(x1,coef(l1)[1]+x1*coef(l1)[2],type="l",lwd=3)
text(1.5, y1[11], expression(Y[X==0]),col="grey60")
text(11.5, y1[21], expression(Y[X==10]),col="grey60")
text(-11.5, y1[1], expression(Y[X==-10]),col="grey60")
```

The best fitting straight line would be called a statistical model of the linear relationship between the observed values of $X$ and $Y$. It can be obtained by fitting a General Linear Model (GLM) to the data. If $X$ were to represent repeated measurements the multivariate GLM for repeated measures would have to be fitted to the data. This can be very problematic, because statistical models rely on [Ergodic theory](https://en.wikipedia.org/wiki/Ergodic_theory): 

> "... it is the study of the long term average behavior of systems evolving in time."


```{block2, type="rmdkennen"}
In other words: If you throw 1 die 100 times in a row, the average of the 100 numbers is the **time-average** of one of the observables of die-throwing systems. If this system is ergodic, then its **time-average** is expected to be similar to the average of the numbers that turn up if you throw 100 dice all at the same instance of time. The dice layed out on the table represent a spatial sample, a snapshot frozen in time, of the possible states the system can be in. Taking the average would be the **spatial average** this observable of die-throwing systems. This ergodic condiciotn is often implicitly assumed in Behavioural Science when studies claim to study change by taking different samples of individuals (snapshots of system states) and comparing if they are the same. 
```



 need to assume independence of measurements within and between subjects. These assumptions can be translated to certain conditions that must hold for the model to be valid, known as *Compound Symmetry* and *Sphericity*:    

> The compound symmetry assumption requires that the variances (pooled within-group) and covariances (across subjects) of the different repeated measures are homogeneous (identical). This is a sufficient condition for the univariate F test for repeated measures to be valid (i.e., for the reported F values to actually follow the F distribution). However, it is not a necessary condition. The sphericity assumption is a necessary and sufficient condition for the F test to be valid; it states that the within-subject "model" consists of independent (orthogonal) components. The nature of these assumptions, and the effects of violations are usually not well-described in ANOVA textbooks; [^assumptions]   

As you can read in the quoted text above, these conditions must hold in order to be able to identify unique independent components as the sources of variation of $Y$ over time within a subject. This is the a clear example of:


If you choose to use GLM repeated measures to model change over time, you will only be able to infer independent components that are responsible for the time-evolution of $Y$. As is hinted in the last sentence of the quote, the validity of such inferences is not a common topic of discussion statistics textbooks.

### **No! ... It's a time series!**

The important difference between a regular 2-dimensional Euclidean plane and the space in which we model change processes is that the $X$-axis represents the physical dimension **time**. In the case of the Linear Map we have a 1D space with one 'spatial' dimension $Y$ and a time dimension $t$. This  is called time series if $Y$ is sampled as a continuous process, or a trial series if the time between subsequent observations is not relevant, just the fact that there was a temporal order (for example, a series of response latencies to trials in a psychological experiment in the order in which they were presented to the subject).

```{r, echo=FALSE}
plot(0:20,y1, type="b", lwd=2, xlim=c(-2,22), yaxt="n", xlab="Time / Trial series", ylab="Y", main = "1D Euclidean Space")
abline(h=0,v=0,lty=2)
x2 <- (x1+10)
l2 <- lm(y1~x2)
lines(x2,coef(l2)[1]+x2*coef(l2)[2],type="l",lwd=3)
text(-1.2, y1[1], expression(Y[t==0]),col="grey60")
text(21.5, y1[21], expression(Y[t==20]),col="grey60")
```

Time behaves different from a spatial dimension in that it is directional (time cannot be reversed), it cannot take on negative values, and, unless one is dealing with a truly random process, there will be a temporal correlation across one or more values of $Y$ seperated by an amount of time. In the linear difference equation this occurs because each value one step in the future is calculated based on the current value. If the values of $Y$ represent an observable of a dynamical system, the system can be said to have a history, or a memory.

Ergodic systems do *not* have a history or a memory that extends across more than one time step. This is very convenient, because one can calculate the expected value of a system observable given infinite time, by making use of of the laws of probabilities of random events (or random fields). This means: The average of an observable of an Ergodic system measured across infinite time (its entire history, the **time-average**), will be the be the same value as the average of this observable measured at one instance in time, but in an infinite amount of systems of the same kind (the population, the **spatial average**) [^dice]. 

The simple linear difference equation will have a form of *perfect memory* across the smallest time scale (i.e., the increment of 1, $t+1$). This 'memory' just concerns a correlation of 1 between values at adjacent time points (a short range temporal correlation, SRC), because the change from $Y_t$ to $Y_{t+1}$ is exactly equal to $a * Y_t$ at each iteration step. This is the meaning of deterministic, not that each value of $Y$ is the same, but that the value of $Y$ now can be perfectly explained form the value of $Y$ one moment in the past.

Summarising, the most profound difference is not the fact that the equation of linear change is a deterministic model and the GLM is a probabilistic model with parameters fitted from data, this is something we can (and will) do for $a$ as well. The profound difference between the models is the role given to the passage of time: 

* The linear difference equation represents changes in $Y$ as a function of the physical dimension *time* and $Y$ itself.
* The GLM represents changes in $Y$ as a function of a [linear predictor](https://en.wikipedia.org/wiki/Linear_predictor_function) composed of additive components that can be regarded as independent sources of variation that sum up to the observed values of $Y$.




## **Getting started with `R`** 

In this tutorial on [estimating Effect Size Confidence Intervals (ESCI)](http://fredhasselman.com/Onderwijs/OSCblog.html) there are a lot of examples on how to use `R`. It was written as an addendum for [a post](http://centerforopenscience.github.io/osc/2014/03/06/confidence%20intervals/) on the **Open Science Collaboration Blog**, which contains many interesting entries on diverse subjects (like [behavioural priming](http://centerforopenscience.github.io/osc/2014/03/26/behavioral-priming/), [theoretical amnesia](http://centerforopenscience.github.io/osc/2013/11/20/theoretical-amnesia/) and [anonymous peer review](http://centerforopenscience.github.io/osc/2014/05/15/anonymous-peer-review/)) 

## **The `time series` object** {#plotTS}

A time series object is expected to have a time-dimension on the x-axis. This is very convenient, because `R` will generate the time axis for you by looking at the *t*ime *s*eries *p*roperties attribute of the object. Even though we are not working with measurement ourcomes, consider a value at a time-index in a time series object a **sample**:

* `Start` -  The value of time at the first sample in the series (e.g., $0$, or $1905$)
* `End` - The value of time at the last sample in the series (e.g., $100$, or $2005$)
* `Frequency` - The amount of time that passed between two samples, or, the sample rate (e.g., $0.5$, or $10$)

Examples of using the time series object.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Get a timeseries of 100 random numbers 
Y <- ts(rnorm(100))
# plot.ts
plot(Y)
# Get sample rate info
tsp(Y)
# Extract the time vector
time(Y)
```

For now, these values are in principle all arbitrary units (`a.u.`). These settings only make sense if they represent the parameters of an actual measurement procedure.

It is easy to adjust the time vector, by assigning new values using `tsp()` (values have to be possible given the timeseries length). For example, suppose the sampling frequency was $0.1$ instead of $1$ and the Start time was $10$ and End time was $1000$.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Assign new values
(tsp(Y) <- c(10, 1000, .1))
# Time axis is automatically adjusted 
time(Y)
```

### Plotting a `ts` object as a time series

Depending on which packages you use, there will be different settings applied to time series objects created by `ts()`. Below are some examples of differences between plotting routines.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
require(lattice)       # Needed for plotting
require(latticeExtra)  # Needed for plotting

# stats::plot.ts
plot(Y, lwd = 2, main = "stats::plot.ts")
# lattice::xyplot.ts
xyplot(Y, lwd = 2, main = "lattice::xyplot.ts")
```

### Plotting multiple time series in one figure

Plot multiple timeseries in frames with `plot.ts()` in `package::stats`.
This function takes a matrix as input, here we use `cbind( ... )`.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
# stats::plot.ts  
plot(cbind(Y,
           cumsum(Y), 
           cumsum(c(0,diff(Y)))
           ), 
     yax.flip = TRUE, col = "blue", frame.plot = TRUE) 
title(main = expression(paste("Random Numbers: ",N(0,sigma))), 
      xlab = "time (a.u.)")
```

Plot multiple timeseries in one graph with `ts.plot()` in `package::graphics`.
This function can handle multiple `ts` objects as arguments.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
# graphics::ts.plot
ts.plot(Y,
           cumsum(Y), 
           cumsum(c(0,diff(Y))),
        
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("Random Numbers: ",N(0,sigma))),
                     lwd = rep(2,3),
                     lty = c(1:3),
                     col = c("darkred","darkblue","darkgreen")
                     )
        )
legend(0, -10, c("Y","cumsum(Y)", "cumprod(Y)"), lwd = rep(2,3), lty = c(1:3), col = c("darkred","darkblue","darkgreen"), merge = TRUE, cex=.9)
```

Use `xyplot()` in `package::lattice` to create a plot with panels. The easiest way to do this is to create a dataset in so-called "long" format. This means the variable to plot is in 1 column and other variables indicate different levels, or conditions under which the variable was observed or simulated.

Function `ldply()` is used to generate $Y$ for three different settings of $r$. The values of $r$ are passed as a **l**ist and after a function is applied the result is returned as a **d**ataframe. 
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, tidy=FALSE}
require(plyr)          # Needed for function ldply()

# Create a long format dataframe for various values for `r`
data <- cbind.data.frame(Y     = c(as.numeric(Y), cumsum(Y), cumsum(c(0,diff(Y)))),
                         time  = c(time(Y), time(Y), time(Y)),
                         label = factor(c(rep("Y",length(Y)),  rep("cumsum(Y)",length(Y)), rep("cumsum(diff(Y))",length(Y))))
                         )
# Plot using the formula interface
xyplot(Y ~ time | label, data = data, type = "l", main = expression(paste("Random Numbers: ",N(0,sigma))))
```

## The return plot

To create a return plot the values of $Y$ have to be shifted by a certain lag. The functions `lead()` and `lag()` in `package::dplyr` are excellent for this purpose (note that `dplyr::lag()` behaves different from `stats::lag()`).
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache=FALSE, collapse=TRUE, tidy=FALSE}
# Function lag() and lead()
require(dplyr)

# Get exponential growth
Y <- rnorm(1000)
Y1 <- Y/max(Y)
# Get logistic growth in the chaotic regime
Y2 <- cumsum(c(0,diff(Y)))/max(cumsum(c(0,diff(Y))))
# Use the `lag` function from package `dplyr`
op <- par(mfrow = c(1,2), pty = "s")
plot(dplyr::lag(Y1), Y1, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = "Y(t+1)",
     main = "rnorm(1000) / max")
plot(dplyr::lag(Y2), Y2, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = "Y(t+1)",
     main = "cumsum(diff(Y)) / max")
par(op)
```

Use `l_ply()` from `package::plyr` to create return plots with different lags. The **l_** before **ply** means the function will take a **l**ist as input to a function, but it will not expect any data to be returned, for example in the case of a function that is used to plot something.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=FALSE, figure.height=20, figure.width=20, tidy=FALSE}
# Explore different lags
op <- par(mfrow = c(1,2), pty = "s")
plyr::l_ply(1:4, function(l) plot(lag(Y2, n = l), Y2, xy.labels = FALSE, pch = 16, xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = paste0("Y(t+",l,")"), cex = .8))
par(op)
```

## Spreadsheet versus `Matlab`, `R`, etc.

Coding change processes (difference equations) in `Matlab` and `R` is always easier than using a spreadsheet. One obvious way to do it is to use a counter variable representing the iterations of time in a `for ... next` loop. The iterations should run over a vector (which is the same concept as a row or a column in a spreadsheet: An indexed array of numbers or characters). The first entry should be the starting value, so the vector index $1$ represents $Y_0$.

The loop can be implemented a number of ways, for example as a function which can be called from a script or the command / console window. In `R` working with functions is easy, and very much recommended, because it will speed up calculations considerably, and it will reduce the amount of code you need to write. You need to gain some experience with coding in `R` before you'll get it right. In order to get it lean and clean (and possibly even mean as well) you'll need a lot of experience with coding in `R`,therefore, we will (eventually) provide you the functions you'll need to complete the assignments. All you have to do is figure out how to use, or modify them to suit your specific needs.





-----
[^assumptions]: [Retreived from www.statsoft.com](https://www.statsoft.com/Textbook/ANOVA-MANOVA#sphericity)
[^einstein]: Einstein as quoted by Heisenberg.
[^ergodic]: See  Dajani &  Dirksin (2008, p. 5, ["A simple introduction to Ergodic Theory"](http://www.staff.science.uu.nl/~kraai101/lecturenotes2009.pdf))
